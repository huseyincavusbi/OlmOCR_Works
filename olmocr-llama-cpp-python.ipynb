{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2025-07-24T20:21:31.928966Z",
     "iopub.status.busy": "2025-07-24T20:21:31.928639Z",
     "iopub.status.idle": "2025-07-24T20:21:35.220988Z",
     "shell.execute_reply": "2025-07-24T20:21:35.220222Z",
     "shell.execute_reply.started": "2025-07-24T20:21:31.928947Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.14.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2025-07-24T20:21:35.222724Z",
     "iopub.status.busy": "2025-07-24T20:21:35.222483Z",
     "iopub.status.idle": "2025-07-24T20:21:35.228291Z",
     "shell.execute_reply": "2025-07-24T20:21:35.227379Z",
     "shell.execute_reply.started": "2025-07-24T20:21:35.222702Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!pip install llama-cpp-python\\n\\nfrom llama_cpp import Llama\\n\\nllm = Llama.from_pretrained(\\n\\trepo_id=\"lmstudio-community/olmOCR-7B-0225-preview-GGUF\",\\n\\tfilename=\"olmOCR-7B-0225-preview-Q4_K_M.gguf\",\\n)\\n'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"!pip install llama-cpp-python\n",
    "\n",
    "from llama_cpp import Llama\n",
    "\n",
    "llm = Llama.from_pretrained(\n",
    "\trepo_id=\"lmstudio-community/olmOCR-7B-0225-preview-GGUF\",\n",
    "\tfilename=\"olmOCR-7B-0225-preview-Q4_K_M.gguf\",\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T20:27:12.095009Z",
     "iopub.status.busy": "2025-07-24T20:27:12.094253Z",
     "iopub.status.idle": "2025-07-24T20:27:16.518160Z",
     "shell.execute_reply": "2025-07-24T20:27:16.517500Z",
     "shell.execute_reply.started": "2025-07-24T20:27:12.094976Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading GGUF model with the correct 'qwen' chat format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_context: n_ctx_per_seq (2048) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
      "llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "from llama_cpp import Llama\n",
    "\n",
    "print(\"Loading GGUF model with the correct 'qwen' chat format...\")\n",
    "llm = Llama.from_pretrained(\n",
    "    repo_id=\"lmstudio-community/olmOCR-7B-0225-preview-GGUF\",\n",
    "    filename=\"olmOCR-7B-0225-preview-Q4_K_M.gguf\",\n",
    "    chat_format=\"qwen\",\n",
    "    n_ctx=2048,\n",
    "    verbose=False\n",
    ")\n",
    "print(\"Model loaded successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab_type": "code",
    "execution": {
     "iopub.execute_input": "2025-07-24T20:21:40.163243Z",
     "iopub.status.busy": "2025-07-24T20:21:40.163036Z",
     "iopub.status.idle": "2025-07-24T20:21:40.167792Z",
     "shell.execute_reply": "2025-07-24T20:21:40.167090Z",
     "shell.execute_reply.started": "2025-07-24T20:21:40.163226Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'llm.create_chat_completion(\\n\\tmessages = [\\n\\t\\t{\\n\\t\\t\\t\"role\": \"user\",\\n\\t\\t\\t\"content\": [\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\"type\": \"text\",\\n\\t\\t\\t\\t\\t\"text\": \"Describe this image in one sentence.\"\\n\\t\\t\\t\\t},\\n\\t\\t\\t\\t{\\n\\t\\t\\t\\t\\t\"type\": \"image_url\",\\n\\t\\t\\t\\t\\t\"image_url\": {\\n\\t\\t\\t\\t\\t\\t\"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\\n\\t\\t\\t\\t\\t}\\n\\t\\t\\t\\t}\\n\\t\\t\\t]\\n\\t\\t}\\n\\t]\\n)'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"llm.create_chat_completion(\n",
    "\tmessages = [\n",
    "\t\t{\n",
    "\t\t\t\"role\": \"user\",\n",
    "\t\t\t\"content\": [\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"type\": \"text\",\n",
    "\t\t\t\t\t\"text\": \"Describe this image in one sentence.\"\n",
    "\t\t\t\t},\n",
    "\t\t\t\t{\n",
    "\t\t\t\t\t\"type\": \"image_url\",\n",
    "\t\t\t\t\t\"image_url\": {\n",
    "\t\t\t\t\t\t\"url\": \"https://cdn.britannica.com/61/93061-050-99147DCE/Statue-of-Liberty-Island-New-York-Bay.jpg\"\n",
    "\t\t\t\t\t}\n",
    "\t\t\t\t}\n",
    "\t\t\t]\n",
    "\t\t}\n",
    "\t]\n",
    ")\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T20:26:22.454011Z",
     "iopub.status.busy": "2025-07-24T20:26:22.453740Z",
     "iopub.status.idle": "2025-07-24T20:26:22.460121Z",
     "shell.execute_reply": "2025-07-24T20:26:22.459464Z",
     "shell.execute_reply.started": "2025-07-24T20:26:22.453994Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully found 11 files in the directory.\n",
      "First 5 files: ['IMG_1325.JPG', 'IMG_1317.JPG', 'IMG_1320.JPG', 'IMG_1319.JPG', 'IMG_1321.JPG']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "image_folder_path = \"/kaggle/input/gltnnn/Gltn\"\n",
    "\n",
    "# Let's list the files to confirm the images are there\n",
    "try:\n",
    "    file_list = os.listdir(image_folder_path)\n",
    "    if len(file_list) > 0:\n",
    "        print(f\"Successfully found {len(file_list)} files in the directory.\")\n",
    "        print(\"First 5 files:\", file_list[:5])\n",
    "    else:\n",
    "        print(\"The directory is empty. Please check the path and your upload.\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The directory '{image_folder_path}' was not found. Please double-check your dataset path.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T20:21:40.206427Z",
     "iopub.status.busy": "2025-07-24T20:21:40.206237Z",
     "iopub.status.idle": "2025-07-24T20:21:43.411563Z",
     "shell.execute_reply": "2025-07-24T20:21:43.410867Z",
     "shell.execute_reply.started": "2025-07-24T20:21:40.206407Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-cpp-python in /usr/local/lib/python3.11/dist-packages (0.3.14)\n",
      "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (11.2.1)\n",
      "Requirement already satisfied: fpdf2 in /usr/local/lib/python3.11/dist-packages (2.8.3)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.14.0)\n",
      "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (1.26.4)\n",
      "Requirement already satisfied: diskcache>=5.6.1 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (5.6.3)\n",
      "Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n",
      "Requirement already satisfied: defusedxml in /usr/local/lib/python3.11/dist-packages (from fpdf2) (0.7.1)\n",
      "Requirement already satisfied: fonttools>=4.34.0 in /usr/local/lib/python3.11/dist-packages (from fpdf2) (4.58.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n",
      "Requirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.3.8)\n",
      "Requirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (1.2.4)\n",
      "Requirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (0.1.1)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2025.2.0)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2022.2.0)\n",
      "Requirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.20.0->llama-cpp-python) (2.4.1)\n",
      "Requirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\n",
      "Requirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.20.0->llama-cpp-python) (2022.2.0)\n",
      "Requirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.20.0->llama-cpp-python) (1.4.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\n",
      "Requirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.20.0->llama-cpp-python) (2024.2.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install llama-cpp-python Pillow fpdf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T20:39:58.161989Z",
     "iopub.status.busy": "2025-07-24T20:39:58.161272Z",
     "iopub.status.idle": "2025-07-24T20:40:33.070339Z",
     "shell.execute_reply": "2025-07-24T20:40:33.069582Z",
     "shell.execute_reply.started": "2025-07-24T20:39:58.161956Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 11 images to process.\n",
      "Starting OCR processing with the most direct prompt...\n",
      "Processed: IMG_1317.JPG\n",
      "Processed: IMG_1318.JPG\n",
      "Processed: IMG_1319.JPG\n",
      "Processed: IMG_1320.JPG\n",
      "Processed: IMG_1321.JPG\n",
      "Processed: IMG_1322.JPG\n",
      "Processed: IMG_1323.JPG\n",
      "Processed: IMG_1324.JPG\n",
      "Processed: IMG_1325.JPG\n",
      "Processed: IMG_1326.JPG\n",
      "Processed: IMG_1327.JPG\n",
      "OCR processing complete.\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "from io import BytesIO\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "def image_to_base64(image, format=\"JPEG\"):\n",
    "    \"\"\"Converts a PIL Image to a base64 encoded string.\"\"\"\n",
    "    buffered = BytesIO()\n",
    "    image.save(buffered, format=format)\n",
    "    return base64.b64encode(buffered.getvalue()).decode('utf-8')\n",
    "\n",
    "image_folder = image_folder_path\n",
    "all_files = os.listdir(image_folder)\n",
    "image_files = sorted([\n",
    "    os.path.join(image_folder, f) for f in all_files\n",
    "    if f.lower().endswith(('.png', '.jpg', '.jpeg'))\n",
    "])\n",
    "\n",
    "if not image_files:\n",
    "    print(\"CRITICAL ERROR: No image files found.\")\n",
    "else:\n",
    "    print(f\"Found {len(image_files)} images to process.\")\n",
    "\n",
    "processed_images = []\n",
    "\n",
    "print(\"Starting OCR processing with the most direct prompt...\")\n",
    "for image_path in image_files:\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "    image_b64 = image_to_base64(image)\n",
    "\n",
    "    # Use the simplest, most direct command possible.\n",
    "    prompt_text = \"ocr\"\n",
    "\n",
    "    response = llm.create_chat_completion(\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"image_url\", \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_b64}\"}},\n",
    "                    # Add the direct command as text.\n",
    "                    {\"type\": \"text\", \"text\": prompt_text}\n",
    "                ]\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "    decoded_output = response['choices'][0]['message']['content']\n",
    "\n",
    "    processed_images.append({\n",
    "        \"image_path\": image_path,\n",
    "        \"text\": decoded_output or \" \"\n",
    "    })\n",
    "    print(f\"Processed: {os.path.basename(image_path)}\")\n",
    "\n",
    "print(\"OCR processing complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-24T20:28:14.805610Z",
     "iopub.status.busy": "2025-07-24T20:28:14.805380Z",
     "iopub.status.idle": "2025-07-24T20:28:14.821132Z",
     "shell.execute_reply": "2025-07-24T20:28:14.820371Z",
     "shell.execute_reply.started": "2025-07-24T20:28:14.805594Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating text-only PDF from 11 processed items...\n",
      "Success! Text-only PDF created at: ocr_text_document.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_36/3172557940.py:11: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font(\"Arial\", size=12)\n",
      "/tmp/ipykernel_36/3172557940.py:24: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font(\"Arial\", 'B', 14)\n",
      "/tmp/ipykernel_36/3172557940.py:25: DeprecationWarning: The parameter \"ln\" is deprecated since v2.5.2. Instead of ln=1 use new_x=XPos.LMARGIN, new_y=YPos.NEXT.\n",
      "  pdf.cell(0, 10, f\"--- Text from: {image_basename} ---\", 0, 1, 'C')\n",
      "/tmp/ipykernel_36/3172557940.py:28: DeprecationWarning: Substituting font arial by core font helvetica - This is deprecated since v2.7.8, and will soon be removed\n",
      "  pdf.set_font(\"Arial\", size=12)\n",
      "/tmp/ipykernel_36/3172557940.py:32: DeprecationWarning: The parameter \"txt\" has been renamed to \"text\" in 2.7.6\n",
      "  pdf.multi_cell(0, 5, txt=printable_text)\n"
     ]
    }
   ],
   "source": [
    "from fpdf import FPDF\n",
    "import os\n",
    "\n",
    "output_pdf_path = \"ocr_text_document.pdf\"\n",
    "\n",
    "class PDF(FPDF):\n",
    "    def header(self): pass\n",
    "    def footer(self): pass\n",
    "\n",
    "pdf = PDF()\n",
    "pdf.set_font(\"Arial\", size=12)\n",
    "\n",
    "if not processed_images:\n",
    "    print(\"Cannot create PDF, no images were processed.\")\n",
    "else:\n",
    "    print(f\"Creating text-only PDF from {len(processed_images)} processed items...\")\n",
    "\n",
    "    for i, item in enumerate(processed_images):\n",
    "        pdf.add_page()\n",
    "\n",
    "        image_basename = os.path.basename(item[\"image_path\"]) \n",
    "        pdf.set_font(\"Arial\", 'B', 14)\n",
    "        pdf.cell(0, 10, f\"--- Text from: {image_basename} ---\", 0, 1, 'C')\n",
    "        pdf.ln(5)\n",
    "\n",
    "        pdf.set_font(\"Arial\", size=12)\n",
    "        ocr_text = item[\"text\"]\n",
    "        printable_text = ocr_text.encode('latin-1', 'replace').decode('latin-1')\n",
    "\n",
    "        pdf.multi_cell(0, 5, txt=printable_text)\n",
    "\n",
    "    pdf.output(output_pdf_path)\n",
    "    print(f\"Success! Text-only PDF created at: {output_pdf_path}\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7936828,
     "sourceId": 12568022,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
